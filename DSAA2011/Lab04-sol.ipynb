{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f16420-62e4-47ed-adb4-f53db4ee05b9",
   "metadata": {},
   "source": [
    "*2025 Spring DSAA 2011 Maching Learning*\n",
    "## Lab Note 04 (Solutions)\n",
    "*Weiwen Chen, Zixin Zhong* \\\n",
    "*Hong Kong University of Science and Technology (Guangzhou)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5ebaa-4d79-4b77-8dd1-d6aedcac7545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b57bccd-9e2f-4b00-b48d-2c53077e8044",
   "metadata": {},
   "source": [
    "**Question 1**. Apply linear classification (affine model) with **only numpy**. Consider dataset  $(\\mathbf{x}_i, y_i), i = 1,2,3,4,5$ with samples \n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\mathbf{x}_1 = -7, \\quad \\mathbf{x}_2= -2,\\quad \\mathbf{x}_3=1, \\quad \\mathbf{x}_4=5, \\quad \\mathbf{x}_5=7  \\\\\n",
    "&y_1 = +1 , \\quad y_2= +1,\\quad y_3= -1, \\quad y_4=-1, \\quad y_5=-1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "1. Write down the design matrix and target vector.\n",
    "1. Estimate $\\bar{\\mathbf{w}}^*$.\n",
    "2. Predict the label of a new test point $x_{\\text{new}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f394f19",
   "metadata": {},
   "source": [
    "**Solution**. Design matrix and target vector are\n",
    "\\begin{align*}\n",
    "\\mathbf{X} = \\begin{bmatrix}\n",
    "1 & -7\\\\\n",
    "1 & -2\\\\\n",
    "1 & 1\\\\\n",
    "1 & 5\\\\\n",
    "1 & 7\n",
    "\\end{bmatrix}\\quad\\text{and}\\quad\\mathbf{y} =\\begin{bmatrix}\n",
    " +1 \\\\  +1 \\\\ -1 \\\\ -1 \\\\ -1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae696c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "X = np.array([[1,-7],[1,-2],[1,1],[1,5],[1,7]]);\n",
    "y = np.array([[1],[1],[-1],[-1],[-1]]);\n",
    "## Linear regression for classification\n",
    "w = inv(X.T@X)@X.T@y;\n",
    "\n",
    "print(\"Estimated w\")\n",
    "print(w)\n",
    "print(\"\\n\")              \n",
    "              \n",
    "Xt = np.array([[1,-2]])\n",
    "y_predict = Xt @ w\n",
    "print(\"Predicted y\")\n",
    "print(y_predict)\n",
    "print(\"\\n\")           \n",
    "\n",
    "y_class_predict = np.sign(y_predict)\n",
    "print(\"Predicted y class\")\n",
    "print(y_class_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfbf20-1a73-4d9c-ad89-c78598d89232",
   "metadata": {},
   "source": [
    "**Question 2**. \n",
    "1. Train a polynomial classification model with dataset in *Question 1* and predict for new test points.\n",
    "<br> Set the order of the polynomial function as $2$ and $3$ individually.\n",
    "1. Simulate a toy dataset (e.g. using *numpy.random*), train a polynomial classification model with the simulated dataset.\n",
    "<br> Set the order of the polynomial function as $2$ and $3$ individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ff45c",
   "metadata": {},
   "source": [
    "**Solution for 2-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf8701-4e39-4638-8bd3-d117bca586b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Dataset from Question 1\n",
    "X = np.array([[1, -7], [1, -2], [1, 1], [1, 5], [1, 7]])\n",
    "y = np.array([+1, +1, -1, -1, -1])\n",
    "\n",
    "# Design matrix with polynomial features (degree 2 and 3)\n",
    "def polynomial_features(X, degree):\n",
    "    # Extend features for polynomial terms\n",
    "    return np.hstack([X[:, 1:]**i for i in range(1, degree + 1)])\n",
    "\n",
    "# Step 1: Polynomial features for degree 2 and 3\n",
    "X2 = np.hstack([X[:, :1], polynomial_features(X, degree=2)])  # Add bias term\n",
    "X3 = np.hstack([X[:, :1], polynomial_features(X, degree=3)])  # Add bias term\n",
    "\n",
    "# Step 2: Train models\n",
    "w2 = inv(X2.T @ X2) @ X2.T @ y  # Degree 2 model\n",
    "w3 = inv(X3.T @ X3) @ X3.T @ y  # Degree 3 model\n",
    "\n",
    "print(\"Estimated weights for degree 2:\", w2)\n",
    "print(\"Estimated weights for degree 3:\", w3)\n",
    "\n",
    "# Step 3: Predict for x_new = -2\n",
    "x_new = np.array([-2])\n",
    "X2_new = np.hstack([[1], x_new**1, x_new**2])  # Degree 2 features\n",
    "X3_new = np.hstack([[1], x_new**1, x_new**2, x_new**3])  # Degree 3 features\n",
    "\n",
    "y_pred_2 = np.sign(X2_new @ w2)\n",
    "y_pred_3 = np.sign(X3_new @ w3)\n",
    "\n",
    "print(\"\\nPredicted label for degree 2 model:\", y_pred_2)\n",
    "print(\"Predicted label for degree 3 model:\", y_pred_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f907eb",
   "metadata": {},
   "source": [
    "**Solution for 2-2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d06673",
   "metadata": {},
   "source": [
    "## Using `numpy.random`\n",
    "\n",
    "The `numpy.random` module is used to generate random numbers. It provides functionalities such as random number generation, random sampling, and shuffling arrays.\n",
    "\n",
    "### Common Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate a random float in the range [0, 1)\n",
    "random_float = np.random.rand()\n",
    "\n",
    "# Generate a random array with a specified shape (2x3 matrix)\n",
    "random_array = np.random.rand(2, 3)\n",
    "\n",
    "# Generate random integers in the range [low, high)\n",
    "random_int = np.random.randint(low=10, high=20, size=5)\n",
    "\n",
    "# Sample random numbers from a normal distribution\n",
    "random_normal = np.random.normal(loc=0, scale=1, size=5)\n",
    "\n",
    "# Shuffle an array randomly\n",
    "array = np.array([1, 2, 3, 4, 5])\n",
    "np.random.shuffle(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967dd146",
   "metadata": {},
   "source": [
    "### Applications:\n",
    "* Data augmentation (generating random samples)\n",
    "* Data randomization (shuffling training data)\n",
    "* Simulating random processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Step 1: Simulate the new toy dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "x = np.random.uniform(-5, 5, 6)  # Generate 6 random x values\n",
    "y = np.sign(0.5 * x**2 - 2 * x + 1 + np.random.normal(0, 0.5, 6))  # Quadratic decision boundary with noise\n",
    "\n",
    "# Design matrix with polynomial features (degree 2 and 3)\n",
    "def polynomial_features(x, degree):\n",
    "    return np.vstack([x**i for i in range(degree + 1)]).T\n",
    "\n",
    "# Step 2: Train polynomial models (degree 2 and 3)\n",
    "X2 = polynomial_features(x, degree=2)\n",
    "X3 = polynomial_features(x, degree=3)\n",
    "\n",
    "# Fit linear regression models\n",
    "w2 = inv(X2.T @ X2) @ X2.T @ y  # Degree 2\n",
    "w3 = inv(X3.T @ X3) @ X3.T @ y  # Degree 3\n",
    "\n",
    "print(\"Estimated weights for degree 2:\", w2)\n",
    "print(\"Estimated weights for degree 3:\", w3)\n",
    "\n",
    "# Step 3: Predict for new test points\n",
    "x_new = np.array([-4.5, 0, 4.5])\n",
    "X2_new = polynomial_features(x_new, degree=2)\n",
    "X3_new = polynomial_features(x_new, degree=3)\n",
    "\n",
    "y_pred_2 = np.sign(X2_new @ w2)\n",
    "y_pred_3 = np.sign(X3_new @ w3)\n",
    "\n",
    "print(\"\\nPredicted labels for degree 2 model:\", y_pred_2)\n",
    "print(\"Predicted labels for degree 3 model:\", y_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5a6e0-e7d1-4d91-be20-75ebe099c3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa5a7047-7d4e-4b02-8b1e-35ddba6fc630",
   "metadata": {},
   "source": [
    "**Question 3**. Apply ridge regression to the dataset provided in `DSAA2011-LA04-data.csv'. (Source link: https://www.kaggle.com/datasets/budincsevity/szeged-weather)\n",
    "1. Use Ridge regression model to predict Apparel Temperature (C), with Humidity as the input feature.\n",
    "2. Divide the dataset into a training set and a testing set.\n",
    "3. Compare the effects of different regularization parameters (alpha).\n",
    "4. Calculate mean squared error (MSE) as the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a990b",
   "metadata": {},
   "source": [
    "## Tips:\n",
    "## Basic Functions of pandas\n",
    "\n",
    "pandas is a powerful library for data manipulation and analysis. It is widely used for reading, cleaning, transforming, and analyzing data.\n",
    "\n",
    "### Common Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('DSAA2011-LA04-data.csv')  # Read from a CSV file\n",
    "print(df.head())  # View the first 5 rows\n",
    "\n",
    "# Basic information about the data\n",
    "print(df.info())  # Display dataset information\n",
    "print(df.describe())  # Show statistical information for numerical data\n",
    "\n",
    "# Data selection\n",
    "print(df['Temperature (C)'])  # Select a single column\n",
    "print(df[['Temperature (C)', 'Apparent Temperature (C)']])  # Select multiple columns\n",
    "print(df.iloc[0:5])  # Select rows (first 5 rows)\n",
    "\n",
    "# Data cleaning\n",
    "df = df.dropna()  # Remove missing values\n",
    "df['new_column'] = df['Temperature (C)'] * 2  # Create a new column\n",
    "\n",
    "# Data grouping and aggregation\n",
    "# grouped = df.groupby('Temperature (C)').mean()  # Group by a column and calculate the mean\n",
    "def mean_str(col):\n",
    "    if pd.api.types.is_numeric_dtype(col):\n",
    "        return col.mean()\n",
    "    else:\n",
    "        return col.unique() if col.nunique() == 1 else np.nan\n",
    "# So now you would do something like:\n",
    "\n",
    "grouped = df.groupby('Temperature (C)').agg(mean_str)\n",
    "\n",
    "# # Save data\n",
    "df.to_csv('output.csv', index=False)  # Save to a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc536bf5",
   "metadata": {},
   "source": [
    "### Applications:\n",
    "\n",
    "- Data preprocessing: cleaning, transforming, and preparing data\n",
    "- Data analysis: statistics and visualization\n",
    "- Data import and export: supports various formats (CSV, Excel, SQL, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf70915",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)\n",
    "\n",
    "### What is MSE?\n",
    "\n",
    "Mean Squared Error (MSE) is a common metric used to evaluate model performance, particularly in linear regression and curve fitting. It represents the average squared difference between predicted values and true values.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "If there are true values `y` and predicted values `ŷ`, with a sample size of `n`, the formula for MSE is:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "### Characteristics:\n",
    "- The smaller the MSE, the better the model's predictive performance.\n",
    "- MSE is more sensitive to outliers because it amplifies larger errors by squaring them.\n",
    "\n",
    "### Role in Ridge Regression:\n",
    "\n",
    "In regression, MSE is part of the objective function used to measure the fit of the model. In ridge regression, the MSE is combined with a regularization term, such as $ \\lambda ||w||^2 $ (weighted L2 norm), to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28308a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "data = pd.read_csv(\"DSAA2011-LA04-data.csv\")\n",
    "\n",
    "# Step 3: Data preprocessing\n",
    "# Select feature and target variable\n",
    "X = data[\"Humidity\"].values.reshape(-1, 1)  # Input feature: Humidity\n",
    "y = data[\"Apparent Temperature (C)\"].values  # Target variable: Apparent Temperature (C)\n",
    "\n",
    "# Step 4: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train Ridge regression model\n",
    "alphas = [0.01, 0.1, 1, 10, 100]  # Different regularization parameters\n",
    "mse_train = []\n",
    "mse_test = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)  # Train the model\n",
    "    y_train_pred = ridge.predict(X_train)  # Predictions for the training set\n",
    "    y_test_pred = ridge.predict(X_test)  # Predictions for the testing set\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse_train.append(mean_squared_error(y_train, y_train_pred))\n",
    "    mse_test.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# Step 6: Visualize the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(alphas, mse_train, label=\"Train MSE\", marker='o')\n",
    "plt.plot(alphas, mse_test, label=\"Test MSE\", marker='s')\n",
    "plt.xscale(\"log\")  # Use a logarithmic scale for alpha\n",
    "plt.xlabel(\"Alpha (log scale)\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Effect of Alpha on Ridge Regression\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Print the best results\n",
    "best_alpha = alphas[np.argmin(mse_test)]\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Train MSE: {mse_train[np.argmin(mse_test)]}\")\n",
    "print(f\"Test MSE: {mse_test[np.argmin(mse_test)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ccf3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAA2011",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
