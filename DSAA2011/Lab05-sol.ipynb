{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f16420-62e4-47ed-adb4-f53db4ee05b9",
   "metadata": {},
   "source": [
    "*2025 Spring DSAA 2011 Maching Learning*\n",
    "## Lab Note 05 (Solutions)\n",
    "*Guanghua Li, Weiwen Chen, Zixin Zhong* \\\n",
    "*Hong Kong University of Science and Technology (Guangzhou)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3d58c5-e693-411e-be5d-595d5e2105d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab5b8f-ac49-4b9b-99ba-46ec8cfec10c",
   "metadata": {},
   "source": [
    "**Question 1**. In ridge regression, for any $\\lambda>0$, why is $\\mathbf{X}^\\top \\mathbf{X}+\\lambda \\mathbf{I}$ always invertible?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78f1aa",
   "metadata": {},
   "source": [
    "\n",
    "**Answer**:\n",
    "\n",
    "##### Key Concepts:\n",
    "1. **Ridge Regression Formula**:  \n",
    "   In ridge regression, the goal is to minimize the regularized least squares cost function:\n",
    "   $$\n",
    "   J(\\mathbf{w}) = \\|\\mathbf{y} - \\mathbf{X} \\mathbf{w}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2,\n",
    "   $$\n",
    "   where $\\lambda > 0$ is the regularization parameter, $\\mathbf{X}$ is the design matrix, and $\\mathbf{I}$ is the identity matrix.\n",
    "\n",
    "2. **Invertibility Requirement**:  \n",
    "   The solution to ridge regression involves the term $\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}$, which must be invertible to compute:\n",
    "   $$\n",
    "   \\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y}.\n",
    "   $$\n",
    "\n",
    "3. **Invertibility of Matrices**:  \n",
    "   A square matrix $\\mathbf{A}$ is invertible if it is full rank, meaning its determinant is non-zero, or equivalently, it has no zero eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "##### Proof:\n",
    "Let $\\mathbf{X}^\\top \\mathbf{X}$ be a symmetric matrix of size $n \\times n$. \n",
    "\n",
    "1. **Eigenvalues of $\\mathbf{X}^\\top \\mathbf{X}$**:  \n",
    "   Since $\\mathbf{X}^\\top \\mathbf{X}$ is symmetric and positive semi-definite (all its eigenvalues are non-negative), we denote its eigenvalues as $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ with $\\lambda_i \\geq 0$.\n",
    "\n",
    "2. **Eigenvalues of $\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}$**:  \n",
    "   Adding $\\lambda \\mathbf{I}$ to $\\mathbf{X}^\\top \\mathbf{X}$ shifts all eigenvalues of $\\mathbf{X}^\\top \\mathbf{X}$ by $\\lambda$. The eigenvalues of $\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}$ are:\n",
    "   $$\n",
    "   \\mu_i = \\lambda_i + \\lambda, \\quad \\text{for } i = 1, \\dots, n.\n",
    "   $$\n",
    "\n",
    "   Since $\\lambda > 0$ and $\\lambda_i \\geq 0$, it follows that $\\mu_i > 0$ for all $i$.\n",
    "\n",
    "3. **Conclusion**:  \n",
    "   Since all eigenvalues of $\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}$ are strictly positive, the matrix is positive definite, and therefore invertible.\n",
    "\n",
    "---\n",
    "\n",
    "##### Conclusion:\n",
    "For any $\\lambda > 0$, the matrix $\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}$ is always invertible because it is positive definite, with all eigenvalues strictly greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a3439-b00b-468b-ba84-f6a2875f5db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a6a5f35-ada3-4530-bda4-d896ae5f83aa",
   "metadata": {},
   "source": [
    "**Question 2**. Show that the primal and dual forms of the ridge regression solution are the same, i.e., for any $\\lambda>0$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{d+1}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y}\n",
    "=\n",
    "\\mathbf{X}^{\\top}\\left(\\mathbf{X X}^{\\top}+\\lambda \\mathbf{I}_m\\right)^{-1} \\mathbf{y}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "You use the Woodbury formula\n",
    "$$\n",
    "(\\mathbf{I}+\\mathbf{U}\\mathbf{V})^{-1}=\\mathbf{I}-\\mathbf{U}(\\mathbf{I}+\\mathbf{V}\\mathbf{U})^{-1}\\mathbf{V}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edae5f96-ce72-4adc-9f30-f6840ee5049e",
   "metadata": {},
   "source": [
    "**Solution.** Note that $\\mathbf{X} \\in \\mathbb{R}^{m \\times(d+1)}$. Starting from $\\mathbf{X}^{\\top}\\left(\\mathbf{X X}^{\\top}+\\lambda \\mathbf{I}_m\\right)^{-1} \\mathbf{y}$, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\mathbf{X}^{\\top}\\left(\\mathbf{X X}^{\\top}+\\lambda \\mathbf{I}_m\\right)^{-1} \\mathbf{y} \\\\\n",
    "& =\\lambda^{-1} \\mathbf{X}^{\\top}\\left(\\mathbf{I}_m+\\lambda^{-1} \\mathbf{X} \\mathbf{X}^{\\top}\\right)^{-1} \\mathbf{y} \\\\\n",
    "& =\\lambda^{-1} \\mathbf{X}^{\\top}\\left[\\mathbf{I}_m-\\lambda^{-1} \\mathbf{X}\\left(\\mathbf{I}_{d+1}+\\lambda^{-1} \\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top}\\right] \\mathbf{y} \\\\\n",
    "& =\\lambda^{-1}\\left(\\mathbf{X}^{\\top} \\mathbf{y}-\\mathbf{X}^{\\top} \\mathbf{X}\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{d+1}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y}\\right) \\\\\n",
    "& =\\lambda^{-1}\\left(\\mathbf{I}_{d+1}-\\mathbf{X}^{\\top} \\mathbf{X}\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{d+1}\\right)^{-1}\\right) \\mathbf{X}^{\\top} \\mathbf{y} \\\\\n",
    "& =\\lambda^{-1}\\left[\\mathbf{I}_{d+1}-\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{d+1}\\right)\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{d+1}\\right)^{-1}+\\lambda \\mathbf{I}_{d+1}\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{d+1}\\right)^{-1}\\right] \\mathbf{X}^{\\top} \\mathbf{y} \\\\\n",
    "& =\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{d+1}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The third line follows from the Woodbury matrix identity with the identifications $\\mathbf{U} \\equiv \\lambda^{-1} \\mathbf{X}$ and $\\mathbf{V} \\equiv \\mathbf{X}^{\\top}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5a6e0-e7d1-4d91-be20-75ebe099c3da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cea6d90b-030e-4844-9ac2-20699de84004",
   "metadata": {},
   "source": [
    "**Question 3**. Find the minimum of function\n",
    "    $$\n",
    "        g(\\mathbf{x}) = (\\mathbf{x} - [0\\ x_2\\ 0]^\\top)^\\top \\mathbf{x} + [1\\ 2 \\ 0] \\cdot \\mathbf{x} + 5, \\ \\mathbf{x}=\\{x_1, x_2, x_3\\}^\\top \\in \\mathbb{R}_{3\\times 1}\n",
    "    $$\n",
    "    with gradient descent.\n",
    "\n",
    "Let $\\mathbf{x}_0=[ 0.5\\ 0 \\ 1]^\\top $ and learning rate $\\eta=0.2$:\n",
    "\n",
    "1. What are the values of $\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3$? Obtain your solution **by hand**.\n",
    "2. (Optional) What are the values of $\\mathbf{x}_4, \\mathbf{x}_5$? Obtain your solution **by hand**.\n",
    "3. Use *Python* to calculate the gradient and obtain $\\mathbf{x}_0, \\ldots, \\mathbf{x}_{100}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c04b7b-8cda-4679-88cb-7f88df722ba8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19a7676e",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "---\n",
    "\n",
    "##### **1. Hand Calculation of x₁, x₂, x₃**\n",
    "\n",
    "The function is:\n",
    "\n",
    "$$\n",
    "g(x) = x_1^2 + x_3^2 + x_1 + 2x_2 + 5\n",
    "$$\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_g(x) = \\begin{bmatrix}\n",
    "2x_1 + 1 \\\\\n",
    "2 \\\\\n",
    "2x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Initial point:**\n",
    "\n",
    "$$\n",
    "x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Learning rate:\n",
    "\n",
    "$$\n",
    "\\eta = 0.2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Iteration 1 (\\(k = 0\\)):**\n",
    "\n",
    "$$\n",
    "x^{(1)} = x^{(0)} - \\eta \\cdot \\nabla g(x^{(0)})\n",
    "$$\n",
    "\n",
    "<!-- Substitute $x^{(0)}$ = \\begin{bmatrix} 0.5 \\\\ 0 \\\\ 1 \\end{bmatrix}\\: -->\n",
    "Substitute $x^{(0)} = [0.5\\ 0\\ 1]^\\top$:\n",
    "\n",
    "<!-- $$\n",
    "\\nabla_g(x^{(0)}) = \\begin{bmatrix}\n",
    "2(0.5) + 1 \\\\\n",
    "2 \\\\\n",
    "2(1)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "$$ -->\n",
    "\n",
    "$$\n",
    "\\nabla_g(x^{(0)}) = \\begin{bmatrix}\n",
    "2 \\times 0.5 + 1 \\\\\n",
    "2 \\\\\n",
    "2 \\times 1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "2 \\\\\n",
    "2 \\\\\n",
    "2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^{(1)} = \\begin{bmatrix} 0.5 \\\\ 0 \\\\ 1 \\end{bmatrix} - 0.2 \\cdot \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "0.5 - 0.4 \\\\\n",
    "0 - 0.4 \\\\\n",
    "1 - 0.4\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "-0.4 \\\\\n",
    "0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "x^{(1)} = \\begin{bmatrix} 0.1 \\\\ -0.4 \\\\ 0.6 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Iteration 2 (\\(k = 1\\)):**\n",
    "\n",
    "$$\n",
    "x^{(2)} = x^{(1)} - \\eta \\cdot \\nabla g(x^{(1)})\n",
    "$$\n",
    "\n",
    "Substitute $x^{(1)} = [0.1\\ -0.4\\ 0.6]^\\top$:\n",
    "\n",
    "<!-- $$\n",
    "\\nabla_g(x^{(1)}) = \\begin{bmatrix}\n",
    "2(0.1) + 1 \\\\\n",
    "2 \\\\\n",
    "2(0.6)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1.2 \\\\\n",
    "2 \\\\\n",
    "1.2\n",
    "\\end{bmatrix}\n",
    "$$ -->\n",
    "\n",
    "$$\n",
    "\\nabla_g(x^{(1)}) = \\begin{bmatrix}\n",
    "2 \\times 0.1 + 1 \\\\\n",
    "2 \\\\\n",
    "2 \\times 0.6\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1.2 \\\\\n",
    "2 \\\\\n",
    "1.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^{(2)} = \\begin{bmatrix} 0.1 \\\\ -0.4 \\\\ 0.6 \\end{bmatrix} - 0.2 \\cdot \\begin{bmatrix} 1.2 \\\\ 2 \\\\ 1.2 \\end{bmatrix} = \\begin{bmatrix}\n",
    "0.1 - 0.24 \\\\\n",
    "-0.4 - 0.4 \\\\\n",
    "0.6 - 0.24\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "-0.14 \\\\\n",
    "-0.8 \\\\\n",
    "0.36\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "x^{(2)} = \\begin{bmatrix} -0.14 \\\\ -0.8 \\\\ 0.36 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Iteration 3 (\\(k = 2\\)):**\n",
    "\n",
    "$$\n",
    "x^{(3)} = x^{(2)} - \\eta \\cdot \\nabla g(x^{(2)})\n",
    "$$\n",
    "\n",
    "Substitute $x^{(2)} = [0.14\\ -0.8\\ 0.36]^\\top$:\n",
    "\n",
    "<!-- $$\n",
    "\\nabla_g(x^{(2)}) = \\begin{bmatrix}\n",
    "2(-0.14) + 1 \\\\\n",
    "2 \\\\\n",
    "2(0.36)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.72 \\\\\n",
    "2 \\\\\n",
    "0.72\n",
    "\\end{bmatrix}\n",
    "$$ -->\n",
    "\n",
    "$$\n",
    "\\nabla_g(x^{(2)}) = \\begin{bmatrix}\n",
    "2 \\times -0.14 + 1 \\\\\n",
    "2 \\\\\n",
    "2 \\times 0.36\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.72 \\\\\n",
    "2 \\\\\n",
    "0.72\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^{(3)} = \\begin{bmatrix} -0.14 \\\\ -0.8 \\\\ 0.36 \\end{bmatrix} - 0.2 \\cdot \\begin{bmatrix} 0.72 \\\\ 2 \\\\ 0.72 \\end{bmatrix} = \\begin{bmatrix}\n",
    "-0.14 - 0.144 \\\\\n",
    "-0.8 - 0.4 \\\\\n",
    "0.36 - 0.144\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "-0.284 \\\\\n",
    "-1.2 \\\\\n",
    "0.216\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "x^{(3)} = \\begin{bmatrix} -0.284 \\\\ -1.2 \\\\ 0.216 \\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcccdc9",
   "metadata": {},
   "source": [
    "2. (Optional) What are the values of $\\mathbf{x}_4, \\mathbf{x}_5$? Obtain your solution **by hand**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6256f7",
   "metadata": {},
   "source": [
    "We continue from $x^{(3)} = [-0.284\\ -1.2\\ 0.216]^\\top$ and follow the same process as before to compute \\$x^{(4)}$ and $x^{(5)}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Iteration 4 (\\(k = 3\\)):**\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla g(x^{(3)}) = \\begin{bmatrix}\n",
    "2 \\times -0.284 + 1 \\\\\n",
    "2 \\\\\n",
    "2 \\times 0.216\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.432 \\\\\n",
    "2 \\\\\n",
    "0.432\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\n",
    "x^{(4)} = x^{(3)} - \\eta \\cdot \\nabla g(x^{(3)})\n",
    "$$\n",
    "\n",
    "Substitute $x^{(3)} = [-0.284\\ -1.2\\ 0.216]^\\top$ and $\\eta = 0.2$:\n",
    "\n",
    "$$\n",
    "x^{(4)} = \\begin{bmatrix} -0.284 \\\\ -1.2 \\\\ 0.216 \\end{bmatrix} - 0.2 \\cdot \\begin{bmatrix} 0.432 \\\\ 2 \\\\ 0.432 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^{(4)} = \\begin{bmatrix}\n",
    "-0.284 - 0.0864 \\\\\n",
    "-1.2 - 0.4 \\\\\n",
    "0.216 - 0.0864\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "-0.3704 \\\\\n",
    "-1.6 \\\\\n",
    "0.1296\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "x^{(4)} = \\begin{bmatrix} -0.3704 \\\\ -1.6 \\\\ 0.1296 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Iteration 5 (\\(k = 4\\)):**\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla g(x^{(4)}) = \\begin{bmatrix}\n",
    "2 \\times -0.3704 + 1 \\\\\n",
    "2 \\\\\n",
    "2 \\times 0.1296\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.2592 \\\\\n",
    "2 \\\\\n",
    "0.2592\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\n",
    "x^{(5)} = x^{(4)} - \\eta \\cdot \\nabla g(x^{(4)})\n",
    "$$\n",
    "\n",
    "Substitute $x^{(4)} = [-0.3704\\ -1.6\\ 0.1296]^\\top$ and $\\eta = 0.2\\$:\n",
    "\n",
    "$$\n",
    "x^{(5)} = \\begin{bmatrix} -0.3704 \\\\ -1.6 \\\\ 0.1296 \\end{bmatrix} - 0.2 \\cdot \\begin{bmatrix} 0.2592 \\\\ 2 \\\\ 0.2592 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^{(5)} = \\begin{bmatrix}\n",
    "-0.3704 - 0.05184 \\\\\n",
    "-1.6 - 0.4 \\\\\n",
    "0.1296 - 0.05184\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "-0.42224 \\\\\n",
    "-2.0 \\\\\n",
    "0.07776\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "x^{(5)} = \\begin{bmatrix} -0.42224 \\\\ -2.0 \\\\ 0.07776 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### **Summary of Results**\n",
    "\n",
    "- $x^{(4)} = [-0.3704\\ -1.6\\ 0.1296]^\\top$\n",
    "- $x^{(5)} = [-0.42224\\ -2.0\\ 0.07776]^\\top$\n",
    "\n",
    "You can continue this process for further iterations if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a4b30",
   "metadata": {},
   "source": [
    "3. Use *Python* to calculate the gradient and obtain $\\mathbf{x}_0, \\ldots, \\mathbf{x}_{100}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ff4368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x^0 = [0.5 0.  1. ]\n",
      "x^1 = [ 0.1 -0.4  0.6]\n",
      "x^2 = [-0.14 -0.8   0.36]\n",
      "x^3 = [-0.284 -1.2    0.216]\n",
      "x^4 = [-0.3704 -1.6     0.1296]\n",
      "x^5 = [-0.42224 -2.       0.07776]\n",
      "x^6 = [-0.453344 -2.4       0.046656]\n",
      "x^7 = [-0.4720064 -2.8        0.0279936]\n",
      "x^8 = [-0.48320384 -3.2         0.01679616]\n",
      "x^9 = [-0.4899223 -3.6        0.0100777]\n",
      "\n",
      "Final result after 100 iterations:\n",
      "x^100 = [-5.00000000e-01 -4.00000000e+01  6.53318624e-23]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the function gradient\n",
    "def gradient(x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of g(x).\n",
    "    g(x) = x1^2 + x3^2 + x1 + 2 * x_2 + 5\n",
    "    ∇g(x) = [2 * x_1 + 1, 2, 2 * x_3]\n",
    "    \"\"\"\n",
    "    return np.array([2 * x[0] + 1, 2, 2 * x[2]])\n",
    "\n",
    "# Step 2: Initialize parameters\n",
    "x = np.array([0.5, 0, 1])  # Initial point x₀\n",
    "eta = 0.2  # Learning rate\n",
    "iterations = 100  # Number of iterations\n",
    "x_values = [x]  # Store all x values\n",
    "\n",
    "# Step 3: Perform gradient descent\n",
    "for _ in range(iterations):\n",
    "    grad = gradient(x)  # Calculate the gradient\n",
    "    x = x - eta * grad  # Update x using the gradient descent formula\n",
    "    x_values.append(x)  # Store the updated x\n",
    "\n",
    "# Step 4: Print results\n",
    "for i, x_i in enumerate(x_values[:10]):  # Display the first 10 iterations\n",
    "    print(f\"x^{i} = {x_i}\")\n",
    "\n",
    "# Print the final result after 100 iterations\n",
    "print(f\"\\nFinal result after {iterations} iterations:\")\n",
    "print(f\"x^{iterations} = {x_values[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be298e8-959b-4e9d-b2b9-d678b128dd84",
   "metadata": {},
   "source": [
    "**Question 4**. Find the minimum of function\n",
    "    $$\n",
    "        g(\\mathbf{x}) = (\\mathbf{x} - [x_1\\ 0\\ 1]^\\top)^\\top \\mathbf{x} + [1\\ 0 \\ 2] \\cdot \\mathbf{x} + 2, \\ \\mathbf{x}=\\{x_1, x_2, x_3\\}^\\top \\in \\mathbb{R}_{3\\times 1}\n",
    "    $$\n",
    "    with gradient descent with decreasing learning rate.\n",
    "\n",
    "Let the initialization point $\\mathbf{x}_0=[2\\ 1\\ 3]^\\top$ and learning rate $\\eta=0.1$, use *Python* to obtain $\\mathbf{x}_0, \\ldots, \\mathbf{x}_{100}$ and $g(\\mathbf{x}_0), \\ldots, g(\\mathbf{x}_{100})$ with three methods below individually:\n",
    "\n",
    "1. $\\eta_{k+1}=\\eta_k/\\alpha$\n",
    "1. $\\eta_{k+1}=\\eta_k-\\alpha$\n",
    "1. Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04cd9041-76d2-43dd-8a83-809ffc63316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x^0 = [0.5 0.  1. ],               g(x^0) = 4.5\n",
      "x^1 = [0.40909091 0.         0.72727273],               g(x^1) = 3.665289256198347\n",
      "x^2 = [0.32644628 0.         0.52441773],               g(x^2) = 3.1258779686389575\n",
      "x^3 = [0.2513148  0.         0.37048569],               g(x^3) = 2.7590601379249757\n",
      "x^4 = [0.18301346 0.         0.251575  ],               g(x^4) = 2.4978784396992575\n",
      "x^5 = [0.12092132 0.         0.15824121],               g(x^5) = 2.3042028181900633\n",
      "x^6 = [0.06447393 0.         0.08392921],               g(x^6) = 2.155447255317931\n",
      "x^7 = [0.01315812 0.         0.02399961],               g(x^7) = 2.0377337088481613\n",
      "x^8 = [-0.03349262  0.         -0.02489033],               g(x^8) = 1.9422365809875983\n",
      "x^9 = [-0.07590238  0.         -0.0651889 ],               g(x^9) = 1.8631583078075062\n",
      "\n",
      "Final result after 100 iterations:\n",
      "x^100 = [-0.49992743  0.         -0.3170728 ]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: eta_{k+1} = eta_k / alpha\n",
    "\n",
    "# Step 1: Define the function gradient\n",
    "def gradient(x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of g(x).\n",
    "    g(x) = x_2^2 + x_3^2 + x_1 + x_3 + 2\n",
    "    ∇g(x) = [1, 2 * x_2, 2 * x_3 + 1]\n",
    "    \"\"\"\n",
    "    return np.array([1, 2 * x[1], 2 * x[2] + 1])\n",
    "\n",
    "\n",
    "def func_g(x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of g(x).\n",
    "    g(x) = x_2^2 + x_3^2 + x_1 + x_3 + 2\n",
    "    ∇g(x) = [1, 2 * x_2, 2 * x_3 + 1]\n",
    "    \"\"\"\n",
    "    return np.square(x[1]) + np.square(x[2]) + x[0] + x[2] + 2\n",
    "    \n",
    "def eta(previous_eta):\n",
    "    \"\"\"\n",
    "    Calculate the learning rate for the current iteration.\n",
    "    \"\"\"\n",
    "    alpha = 1.1\n",
    "    return previous_eta / alpha\n",
    "\n",
    "\n",
    "# Step 2: Initialize parameters\n",
    "x = np.array([0.5, 0, 1])  # Initial point x_0\n",
    "initial_eta = 0.1  # Initial learning rate\n",
    "iterations = 100  # Number of iterations\n",
    "x_values = [x]  # Store all x values\n",
    "\n",
    "# Step 3: Perform gradient descent\n",
    "previous_eta = initial_eta\n",
    "for _ in range(iterations):\n",
    "    grad = gradient(x)  # Calculate the gradient\n",
    "    cur_eta = eta(previous_eta)\n",
    "    x = x - cur_eta * grad  # Update x using the gradient descent formula\n",
    "    x_values.append(x)  # Store the updated x\n",
    "    previous_eta = cur_eta \n",
    "\n",
    "# Step 4: Print results\n",
    "for i, xi in enumerate(x_values[:10]):  # Display the first 10 iterations\n",
    "    print(f\"x^{i} = {xi},               g(x^{i}) = {func_g(xi)}\")\n",
    "\n",
    "# Print the final result after 100 iterations\n",
    "print(f\"\\nFinal result after {iterations} iterations:\")\n",
    "print(f\"x^{iterations} = {x_values[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee03297-a6c7-4191-9f5b-61988b8d4b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x^0 = [0.5 0.  1. ],               g(x^0) = 4.5\n",
      "x^1 = [0.401 0.    0.703],               g(x^1) = 3.5982089999999998\n",
      "x^2 = [0.303    0.       0.467212],               g(x^2) = 2.988499052944\n",
      "x^3 = [0.206      0.         0.27957287],               g(x^3) = 2.5637338627583284\n",
      "x^4 = [0.11       0.         0.12989488],               g(x^4) = 2.2567675605758533\n",
      "x^5 = [0.015      0.         0.01021485],               g(x^5) = 2.0253191964938173\n",
      "x^6 = [-0.079       0.         -0.08570554],               g(x^6) = 1.8426399002930196\n",
      "x^7 = [-0.172       0.         -0.16276431],               g(x^7) = 1.6917279113745536\n",
      "x^8 = [-0.264       0.         -0.22481568],               g(x^8) = 1.5617264121562147\n",
      "x^9 = [-0.355       0.         -0.27489922],               g(x^9) = 1.445670359807615\n",
      "\n",
      "Final result after 100 iterations:\n",
      "x^100 = [-4.45        0.         -0.49996377]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: eta_{k+1} = eta_k - alpha\n",
    "\n",
    "# Step 1: Define the function gradient\n",
    "def gradient(x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of g(x).\n",
    "    g(x) = x_2^2 + x_3^2 + x_1 + x_3 + 2\n",
    "    ∇g(x) = [1, 2 * x_2, 2 * x_3 + 1]\n",
    "    \"\"\"\n",
    "    return np.array([1, 2 * x[1], 2 * x[2] + 1])\n",
    "\n",
    "def eta(previous_eta):\n",
    "    \"\"\"\n",
    "    Calculate the learning rate for the current iteration.\n",
    "    \"\"\"\n",
    "    alpha = 0.001\n",
    "    return previous_eta - alpha\n",
    "\n",
    "\n",
    "# Step 2: Initialize parameters\n",
    "x = np.array([0.5, 0, 1])  # Initial point x_0\n",
    "initial_eta = 0.1  # Initial learning rate\n",
    "iterations = 100  # Number of iterations\n",
    "x_values = [x]  # Store all x values\n",
    "\n",
    "# Step 3: Perform gradient descent\n",
    "previous_eta = initial_eta\n",
    "for _ in range(iterations):\n",
    "    grad = gradient(x)  # Calculate the gradient\n",
    "    cur_eta = eta(previous_eta)\n",
    "    x = x - cur_eta * grad  # Update x using the gradient descent formula\n",
    "    x_values.append(x)  # Store the updated x\n",
    "    previous_eta = cur_eta \n",
    "\n",
    "# Step 4: Print results\n",
    "for i, xi in enumerate(x_values[:10]):  # Display the first 10 iterations\n",
    "    print(f\"x^{i} = {xi},               g(x^{i}) = {func_g(xi)}\")\n",
    "\n",
    "# Print the final result after 100 iterations\n",
    "print(f\"\\nFinal result after {iterations} iterations:\")\n",
    "print(f\"x^{iterations} = {x_values[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf80cb59-4062-4fcb-a6cf-77dd35bf8492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x^0 = [0.5 0.  1. ],               g(x^0) = 4.5\n",
      "x^1 = [0.34188612 0.         0.52565837],               g(x^1) = 3.1438612264727483\n",
      "x^2 = [0.21367215 0.         0.2626509 ],               g(x^2) = 2.5453085511682674\n",
      "x^3 = [0.09753259 0.         0.08550302],               g(x^3) = 2.1903463719384817\n",
      "x^4 = [-0.01182216  0.         -0.04255205],               g(x^4) = 1.9474364750304383\n",
      "x^5 = [-0.11666738  0.         -0.13847451],               g(x^5) = 1.764033302361528\n",
      "x^6 = [-0.21816948  0.         -0.2118657 ],               g(x^6) = 1.614851895161388\n",
      "x^7 = [-0.31699524  0.         -0.26881589],               g(x^7) = 1.4864508555599627\n",
      "x^8 = [-0.41355811  0.         -0.31346349],               g(x^8) = 1.3712377561897315\n",
      "x^9 = [-0.50813271  0.         -0.34874672],               g(x^9) = 1.2647448458031119\n",
      "\n",
      "Final result after 100 iterations:\n",
      "x^100 = [-6.09097807  0.         -0.49999902]\n"
     ]
    }
   ],
   "source": [
    "# Case 3 (method 1): using the Adagrad algorithm to determine eta\n",
    "\n",
    "# Step 1: Define the function gradient\n",
    "def gradient(x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of g(x).\n",
    "    g(x) = x_2^2 + x_3^2 + x_1 + x_3 + 2\n",
    "    ∇g(x) = [1, 2 * x_2, 2 * x_3 + 1]\n",
    "    \"\"\"\n",
    "    return np.array([1, 2 * x[1], 2 * x[2] + 1])\n",
    "\n",
    "def func_g(x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of g(x).\n",
    "    g(x) = x_2^2 + x_3^2 + x_1 + x_3 + 2\n",
    "    ∇g(x) = [1, 2 * x_2, 2 * x_3 + 1]\n",
    "    \"\"\"\n",
    "    return np.square(x[1]) + np.square(x[2]) + x[0] + x[2] + 2\n",
    "    \n",
    "def eta(initial_eta, previous_grads, epsilon):\n",
    "    \"\"\"\n",
    "    Calculate the learning rate for the current iteration.\n",
    "    \"\"\"\n",
    "    sum_squared_grads = 0\n",
    "    for grad in previous_grads:\n",
    "        sum_squared_grads += np.sum(np.square(grad))\n",
    "    result_eta = initial_eta / (np.sqrt(epsilon + sum_squared_grads))\n",
    "    return result_eta\n",
    "\n",
    "# Step 2: Initialize parameters\n",
    "x = np.array([0.5, 0, 1])  # Initial point x_0\n",
    "initial_eta = 0.5  # Initial learning rate\n",
    "iterations = 100  # Number of iterations\n",
    "epsilon = 1e-6 # epsilon\n",
    "x_values = [x]  # Store all x values\n",
    "\n",
    "previous_grads = []\n",
    "\n",
    "# Step 3: Perform gradient descent\n",
    "for idx in range(iterations):\n",
    "    grad = gradient(x)  # Calculate the gradient\n",
    "    previous_grads.append(grad)\n",
    "    cur_eta = eta(initial_eta, previous_grads, epsilon)\n",
    "    x = x - cur_eta * grad  # Update x using the gradient descent formula\n",
    "    x_values.append(x)  # Store the updated x\n",
    "\n",
    "# Step 4: Print results\n",
    "for i, xi in enumerate(x_values[:10]):  # Display the first 10 iterations\n",
    "    print(f\"x^{i} = {xi},               g(x^{i}) = {func_g(xi)}\")\n",
    "\n",
    "# Print the final result after 100 iterations\n",
    "print(f\"\\nFinal result after {iterations} iterations:\")\n",
    "print(f\"x^{iterations} = {x_values[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d958e8dc-bab9-431d-988f-7de2f7b555d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x^0 = [0.5 0.  1. ],               g(x^0) = 4.5\n",
      "x^1 = [0.34188612 0.         0.52565837],               g(x^1) = 3.1438612264727483\n",
      "x^2 = [0.21367215 0.         0.2626509 ],               g(x^2) = 2.5453085511682674\n",
      "x^3 = [0.09753259 0.         0.08550302],               g(x^3) = 2.1903463719384817\n",
      "x^4 = [-0.01182216  0.         -0.04255205],               g(x^4) = 1.9474364750304383\n",
      "x^5 = [-0.11666738  0.         -0.13847451],               g(x^5) = 1.764033302361528\n",
      "x^6 = [-0.21816948  0.         -0.2118657 ],               g(x^6) = 1.614851895161388\n",
      "x^7 = [-0.31699524  0.         -0.26881589],               g(x^7) = 1.4864508555599627\n",
      "x^8 = [-0.41355811  0.         -0.31346349],               g(x^8) = 1.3712377561897315\n",
      "x^9 = [-0.50813271  0.         -0.34874672],               g(x^9) = 1.2647448458031119\n",
      "\n",
      "Final result after 100 iterations:\n",
      "x^100 = [-6.09097807  0.         -0.49999902]\n"
     ]
    }
   ],
   "source": [
    "# Case 3 (method 2): using the Adagrad algorithm to determine eta\n",
    "\n",
    "# Step 1: Define the function gradient\n",
    "def gradient(x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of g(x).\n",
    "    g(x) = x_2^2 + x_3^2 + x_1 + x_3 + 2\n",
    "    ∇g(x) = [1, 2 * x_2, 2 * x_3 + 1]\n",
    "    \"\"\"\n",
    "    return np.array([1, 2 * x[1], 2 * x[2] + 1])\n",
    "\n",
    "def func_g(x):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of g(x).\n",
    "    g(x) = x_2^2 + x_3^2 + x_1 + x_3 + 2\n",
    "    ∇g(x) = [1, 2 * x_2, 2 * x_3 + 1]\n",
    "    \"\"\"\n",
    "    return np.square(x[1]) + np.square(x[2]) + x[0] + x[2] + 2\n",
    "\n",
    "def eta(initial_eta, previous_grads_sum, epsilon):\n",
    "    \"\"\"\n",
    "    Calculate the learning rate for the current iteration.\n",
    "    \"\"\"\n",
    "    result_eta = initial_eta / (np.sqrt(epsilon + previous_grads_sum))\n",
    "    return result_eta\n",
    "\n",
    "# Step 2: Initialize parameters\n",
    "x = np.array([0.5, 0, 1])  # Initial point x_0\n",
    "initial_eta = 0.5  # Initial learning rate\n",
    "iterations = 100  # Number of iterations\n",
    "epsilon = 1e-6 # epsilon\n",
    "x_values = [x]  # Store all x values\n",
    "\n",
    "previous_grads = []\n",
    "previous_grads_sum = 0\n",
    "\n",
    "# Step 3: Perform gradient descent\n",
    "for idx in range(iterations):\n",
    "    grad = gradient(x)  # Calculate the gradient\n",
    "    previous_grads.append(grad)\n",
    "    previous_grads_sum = previous_grads_sum + np.sum(np.square(grad))\n",
    "    cur_eta = eta(initial_eta, previous_grads_sum, epsilon)\n",
    "    x = x - cur_eta * grad  # Update x using the gradient descent formula\n",
    "    x_values.append(x)  # Store the updated x\n",
    "\n",
    "# Step 4: Print results\n",
    "for i, xi in enumerate(x_values[:10]):  # Display the first 10 iterations\n",
    "    print(f\"x^{i} = {xi},               g(x^{i}) = {func_g(xi)}\")\n",
    "\n",
    "# Print the final result after 100 iterations\n",
    "print(f\"\\nFinal result after {iterations} iterations:\")\n",
    "print(f\"x^{iterations} = {x_values[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAA2011",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
