{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb74499",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17df78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import fowlkes_mallows_score, rand_score, adjusted_rand_score, v_measure_score, \\\n",
    "\tsilhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sklearn as sk\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.preprocessing\n",
    "import os\n",
    "import torch.utils.data as data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa820d",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "We only used the dataset after feature engineering, whose input contains $561$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(root_path, suffix) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X = np.loadtxt(f\"{root_path}/X_{suffix}.txt\")\n",
    "    y = np.loadtxt(f\"{root_path}/y_{suffix}.txt\")\n",
    "    return X, y\n",
    "\n",
    "def load_label_name(root_path) -> dict[int, str] :\n",
    "    with open(f\"{root_path}/activity_labels.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        label_name = {}\n",
    "        for line in lines:\n",
    "            line = line.strip().split()\n",
    "            label_name[int(line[0])] = line[1]\n",
    "    \n",
    "    return label_name\n",
    "\n",
    "def label_binarize(y) -> np.ndarray :\n",
    "    global handler\n",
    "    if \"handler\" not in globals() or handler is None :\n",
    "        handler = LabelBinarizer()\n",
    "        _, y_train = load_data(\"./data/train\", \"train\")\n",
    "        handler.fit(y_train)\n",
    "    \n",
    "    return handler.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae5d51",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Use t-SNE for dimension reduction and visualization. We implemented two functions for figure generations:\n",
    "\n",
    "1. Show directly on $\\texttt{ipynb}$\n",
    "2. Save as $\\texttt{png}$ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_reduction(x, dim_target = 3, algorithm = \"PCA\") :\n",
    "    if algorithm == \"PCA\" :\n",
    "        pca = sk.decomposition.PCA(n_components=dim_target)\n",
    "        x = pca.fit_transform(x)\n",
    "    elif algorithm == \"TSNE\" :\n",
    "        tsne = sk.manifold.TSNE(n_components=dim_target)\n",
    "        x = tsne.fit_transform(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_label_color(label) :\n",
    "    global colormap, y_min, y_max\n",
    "    if \"colormap\" not in globals() :\n",
    "        colormap = matplotlib.colormaps[\"viridis\"]\n",
    "        _, y_train = load_data(\"./data/train\", \"train\")\n",
    "        y_min = np.min(y_train)\n",
    "        y_max = np.max(y_train)\n",
    "\n",
    "    return colormap((label - y_min) / (y_max - y_min))\n",
    "\n",
    "def plot(\n",
    "        x = list[np.ndarray] | np.ndarray, \n",
    "        y = list[np.ndarray] | np.ndarray, \n",
    "        dim_target : int | list[int] = 3, \n",
    "        name : list[str] | str | None = None,\n",
    "        label : dict[str] | None = None,\n",
    "        save_path = None,\n",
    "        ) :\n",
    "    \n",
    "    if type(x) == np.ndarray : x = [x]\n",
    "    if type(y) == np.ndarray : y = [y] * len(x)\n",
    "    if type(dim_target) == int : dim_target = [dim_target] * len(x)\n",
    "    if type(name) == str : name = [name] * len(x)\n",
    "\n",
    "    if len(x) != len(y) or len(x) != len(dim_target) :\n",
    "        raise ValueError(\"x, y, dim_target, need_dimension_reduction must have the same length\")\n",
    "    \n",
    "    nrow = 1 if len(x) <= 3 else 2\n",
    "    ncol = int(math.ceil(len(x) / nrow))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if name is None :\n",
    "        name = [f\"Plot {i}\" for i in range(len(x))]\n",
    "\n",
    "    # write each plot in a different subplot\n",
    "    for i in range(len(x)) :\n",
    "        \n",
    "        if dim_target[i] == 2 :\n",
    "            ax = fig.add_subplot(nrow, ncol, i + 1)\n",
    "            if label is not None :\n",
    "                y_min = np.min(y[i])\n",
    "                y_max = np.max(y[i])\n",
    "\n",
    "                cmap = matplotlib.colormaps['viridis']\n",
    "                colors = [cmap((i - y_min) / (y_max - y_min)) for i in range(int(y_min), int(y_max) + 1)]\n",
    "                for j in range(int(y_min), int(y_max) + 1) :\n",
    "                    ax.scatter(x[i][y[i] == j, 0], x[i][y[i] == j, 1], s=2, color=colors[j - int(y_min)], label=label[j])\n",
    "            else :\n",
    "                ax.scatter(x[i][:, 0], x[i][:, 1], c=y[i], s=2)\n",
    "        elif dim_target[i] == 3 :\n",
    "            ax = fig.add_subplot(nrow, ncol, i + 1, projection='3d')\n",
    "            if label is not None :\n",
    "                y_min = np.min(y[i])\n",
    "                y_max = np.max(y[i])\n",
    "\n",
    "                cmap = matplotlib.colormaps['viridis']\n",
    "                colors = [cmap((i - y_min) / (y_max - y_min)) for i in range(int(y_min), int(y_max) + 1)]\n",
    "                for j in range(int(y_min), int(y_max) + 1) :\n",
    "                    ax.scatter(x[i][y[i] == j, 0], x[i][y[i] == j, 1], x[i][y[i] == j, 2], s=2, color=colors[j - int(y_min)], label=label[j])\n",
    "            else :\n",
    "                ax.scatter(x[i][:, 0], x[i][:, 1], x[i][:, 2], c=y[i], s=2)\n",
    "            \n",
    "            ax.set_zlabel(\"Z\")\n",
    "\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        ax.set_title(name[i])\n",
    "        if label is not None :\n",
    "            ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None :\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    else :\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    X, y = load_data(\"./data/train\", \"train\")\n",
    "    X_tsne = dimension_reduction(X, 2, \"TSNE\")\n",
    "    X_pca = dimension_reduction(X, 2, \"PCA\")\n",
    "    plot([X_tsne], [y], dim_target=2, name=\"tsne\", label=load_label_name(\"./data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67499ec",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Implement usage of two cluster algorithm from $\\texttt{sklearn}$ : $\\text{kmeans}$ and $\\text{hierarchical}$.\n",
    "\n",
    "Use the same function from Visualization part to demonstrate the result of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X : np.ndarray, n_clusters: int, n_init: int = 10) -> tuple :\n",
    "\tkm = KMeans(n_clusters=n_clusters, n_init=n_init, max_iter=600)\n",
    "\n",
    "\tkm.fit(X)\n",
    "\n",
    "\treturn km.labels_, km.cluster_centers_\n",
    "\n",
    "def hierarchical(X : np.ndarray, n_clusters: int) -> tuple :\n",
    "\tfrom sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\tag = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "\n",
    "\tag.fit(X)\n",
    "\n",
    "\treturn ag.labels_, ag.children_\n",
    "\n",
    "def metric(name : str, X : np.ndarray, y : np.ndarray, labels : np.ndarray, centers : np.ndarray) :\n",
    "\tprint(f\"metric of {name}\")\n",
    "\n",
    "\tprint(\"external metric: \")\n",
    "\n",
    "\t# fowlkes_mallows score\n",
    "\tprint(f\"\\tfowlkes_mallows score : {fowlkes_mallows_score(y, labels)}\")\n",
    "\n",
    "\t# rand score\n",
    "\tprint(f\"\\trand score : {rand_score(y, labels)}\")\n",
    "\n",
    "\t# adjusted rand score\n",
    "\tprint(f\"\\tadjusted rand score : {adjusted_rand_score(y, labels)}\")\n",
    "\n",
    "\t# v measure score\n",
    "\tprint(f\"\\tv measure score : {v_measure_score(y, labels)}\")\n",
    "\n",
    "\tprint(\"internal metric: \")\n",
    "\n",
    "\t# silhouette score\n",
    "\tprint(f\"\\tsilhouette score : {silhouette_score(X, labels)}\")\n",
    "\n",
    "\t# calinski harabasz score\n",
    "\tprint(f\"\\tcalinski harabasz score : {calinski_harabasz_score(X, labels)}\")\n",
    "\t# davies bouldin score\n",
    "\tprint(f\"\\tdavies bouldin score : {davies_bouldin_score(X, labels)}\")\n",
    "\n",
    "\tprint()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "\tX, y = load_data(\"./data/train\", \"train\")\n",
    "\n",
    "\tx_dem = dimension_reduction(X, 2, \"TSNE\")\n",
    "\n",
    "\tlabels_kmeans_dem, centers_kmeans_dem = kmeans(x_dem, 6, 20)\n",
    "\t# cluster using the orginal data\n",
    "\tlabels_kmeans_org, centers_kmeans_org = kmeans(X, 6, 20)\n",
    "\n",
    "\tlabels_hierarchical_dem, children_hierarchical_dem = hierarchical(x_dem, 6)\n",
    "\t# cluster using the orginal data\n",
    "\tlabels_hierarchical_org, children_hierarchical_org = hierarchical(X, 6)\n",
    "\t\n",
    "\tmetric(\"kmeans orginal\", X, y, labels_kmeans_org, centers_kmeans_org)\n",
    "\tmetric(\"kmeans dimension reduction\", x_dem, y, labels_kmeans_dem, centers_kmeans_dem)\n",
    "\n",
    "\tmetric(\"hierarchical orginal\", X, y, labels_hierarchical_org, children_hierarchical_org)\n",
    "\tmetric(\"hierarchical dimension reduction\", x_dem, y, labels_hierarchical_dem, children_hierarchical_dem)\n",
    "\n",
    "\t# plot the clusters\n",
    "\tplot(\n",
    "\t\t[x_dem, x_dem, x_dem, x_dem], \n",
    "\t\t[labels_kmeans_org, labels_kmeans_dem, labels_hierarchical_org, labels_hierarchical_dem], \n",
    "\t\tdim_target=2, \n",
    "\t\tname=[\"kmean org\", \"kmean dem\", \"hier org\", \"hier dem\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56502403",
   "metadata": {},
   "source": [
    "# Prediction and Model Choice\n",
    "\n",
    "Select some simple models and ensemble models.\n",
    "\n",
    "Then we choose logistic regression for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "nrow, ncol = 3, 2\n",
    "idx = 0\n",
    "\n",
    "def evaluate(name, \n",
    "\t\t\t model : LogisticRegression | DecisionTreeClassifier | RandomForestClassifier | AdaBoostClassifier | GradientBoostingClassifier,\n",
    "\t\t\t X_test, y_test, time_cost=None, eval_roc : bool = False) :\n",
    "\tglobal best_model, best_accuracy, ncol, nrow, idx\n",
    "\tprint(f\"metric of {name} :\")\n",
    "\n",
    "\ty_pred = model.predict(X_test)\n",
    "\n",
    "\tprint(f\"time cost : {time_cost:.2f} s\")\n",
    "\tacc = accuracy_score(y_test, y_pred)\n",
    "\tprint(f\"accuracy : {acc}\")\n",
    "\t# update best model\n",
    "\tif best_model is None or best_accuracy < acc :\n",
    "\t\tbest_model = model\n",
    "\t\tbest_accuracy = acc\n",
    "\n",
    "\tprint(classification_report(y_test, y_pred))\n",
    "\n",
    "\tprint(f\"confusion matrix :\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "\t# draw AUC and ROC curve\n",
    "\tif eval_roc :\n",
    "\t\tclasses_dict = load_label_name(\"./data\")\n",
    "\t\tclasses = list(classes_dict.keys())\n",
    "\n",
    "\t\ty_test_bin = label_binarize(y_test)\n",
    "\t\ty_score = model.predict_proba(X_test)\n",
    "\n",
    "\t\tfpr, tpr, roc_auc = {}, {}, {}\n",
    "\t\tfor i, cls in enumerate(classes) :\n",
    "\t\t\tfpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "\t\t\troc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\t\t# draw ROC curve\n",
    "\t\tax = fig.add_subplot(nrow, ncol, idx + 1)\n",
    "\t\tax.plot([0, 1], [0, 1], 'k--')\n",
    "\t\tax.set_xlim([0.0, 1.0])\n",
    "\t\tax.set_ylim([0.0, 1.05])\n",
    "\t\tax.set_xlabel('False Positive Rate')\n",
    "\t\tax.set_ylabel('True Positive Rate')\n",
    "\t\tax.set_title(name)\n",
    "\t\tfor i, cls in enumerate(classes) :\n",
    "\t\t\tax.plot(fpr[i], tpr[i], label=f\"{classes_dict[cls]} (AUC {roc_auc[i]:.2f})\", color=get_label_color(cls))\n",
    "\t\tax.legend(loc='lower right')\n",
    "\n",
    "\t\tidx += 1\n",
    "\n",
    "\tprint()\n",
    "def pred_logistic(X_train, y_train, X_test, y_test):\n",
    "\tst_time = time()\n",
    "\t# Logistic Regression\n",
    "\tlr = LogisticRegression(C=1e9, max_iter=1000)\n",
    "\tlr.fit(X_train, y_train)\n",
    "\ttime_cost = time() - st_time\n",
    "\t\n",
    "\tevaluate(\"Logistic Regression\", lr, X_test, y_test, time_cost, eval_roc=True)\n",
    "\t\n",
    "\tevaluate(\"Ridge Regression\", lr, X_test, y_test, time_cost)\n",
    "\n",
    "def pred_dt(X_train, y_train, X_test, y_test):\n",
    "\tst_time = time()\n",
    "\n",
    "\tscaler = StandardScaler()\n",
    "\tX_train = scaler.fit_transform(X_train.data)\n",
    "\tX_test = scaler.transform(X_test.data)\n",
    "\tdt = DecisionTreeClassifier()\n",
    "\tdt.fit(X_train, y_train)\n",
    "\ttime_cost = time() - st_time\n",
    "\t\n",
    "\tevaluate(\"Decision Tree\", dt, X_test, y_test, time_cost, eval_roc=True)\n",
    "\n",
    "def pred_rf(X_train, y_train, X_test, y_test):\n",
    "\tst_time = time()\n",
    "\tscaler = StandardScaler()\n",
    "\tX_train = scaler.fit_transform(X_train.data)\n",
    "\tX_test = scaler.transform(X_test.data)\n",
    "\n",
    "\t# Random Forest\n",
    "\trf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\trf.fit(X_train, y_train)\n",
    "\ttime_cost = time() - st_time\n",
    "\t\n",
    "\tevaluate(\"Random Forest\", rf, X_test, y_test, time_cost, eval_roc=True)\n",
    "\n",
    "def pred_ab(X_train, y_train, X_test, y_test):\n",
    "\tst_time = time()\n",
    "\tscaler = StandardScaler()\n",
    "\tX_train = scaler.fit_transform(X_train.data)\n",
    "\tX_test = scaler.transform(X_test.data)\n",
    "\n",
    "\t# AdaBoost\n",
    "\tab = AdaBoostClassifier(n_estimators=100, estimator=DecisionTreeClassifier(max_depth=3))\n",
    "\tab.fit(X_train, y_train)\n",
    "\ttime_cost = time() - st_time\t\n",
    "\n",
    "\tevaluate(\"AdaBoost\", ab, X_test, y_test, time_cost, eval_roc=True)\n",
    "\n",
    "def pred_svc(X_train, y_train, X_test, y_test):\n",
    "\tst_time = time()\n",
    "\tscaler = StandardScaler()\n",
    "\tX_train = scaler.fit_transform(X_train.data)\n",
    "\tX_test = scaler.transform(X_test.data)\n",
    "\n",
    "\t# SVC\n",
    "\tsvc = SVC(probability=True)\n",
    "\tsvc.fit(X_train, y_train)\n",
    "\ttime_cost = time() - st_time\n",
    "\t\n",
    "\tevaluate(\"SVC\", svc, X_test, y_test, time_cost, eval_roc=True)\n",
    "\n",
    "# finetune the logistic regression\n",
    "def finetune_logistic(X_train, y_train, X_test, y_test):\n",
    "\tlr_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "\tlr_l2 = LogisticRegression(penalty='l2', solver='liblinear', max_iter=1000)\n",
    "\tlr_l2_2 = LogisticRegression(penalty='l2', solver='saga', max_iter=1000)\n",
    "\n",
    "\tst_time = time()\n",
    "\tlr_l1.fit(X_train, y_train)\n",
    "\ttime_cost1 = time() - st_time\n",
    "\tst_time = time()\n",
    "\tlr_l2.fit(X_train, y_train)\n",
    "\ttime_cost2 = time() - st_time\n",
    "\n",
    "\tst_time = time()\n",
    "\tlr_l2_2.fit(X_train, y_train)\n",
    "\ttime_cost2_2 = time() - st_time\n",
    "\n",
    "\tevaluate(\"Logistic Regression L1\", lr_l1, X_test, y_test, time_cost1, eval_roc=False)\n",
    "\tevaluate(\"Logistic Regression L2\", lr_l2, X_test, y_test, time_cost2, eval_roc=True)\n",
    "\tevaluate(\"Logistic Regression L2 (saga)\", lr_l2_2, X_test, y_test, time_cost2_2, eval_roc=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t# Load the data\n",
    "\tX_train, y_train = load_data(\"./data/train\", \"train\")\n",
    "\n",
    "\tX_test, y_test = load_data(\"./data/test\", \"test\")\n",
    "\n",
    "\t# Standardize the data\n",
    "\t\n",
    "\t# Predict using different classifiers\n",
    "\tpred_logistic(X_train, y_train, X_test, y_test)\n",
    "\tpred_svc(X_train, y_train, X_test, y_test)\n",
    "\tpred_dt(X_train, y_train, X_test, y_test)\n",
    "\tpred_rf(X_train, y_train, X_test, y_test)\n",
    "\tpred_ab(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\tprint(f\"best model : {best_model}\")\n",
    "\tprint(f\"best accuracy : {best_accuracy}\")\n",
    "\n",
    "\tprint(f\"finetune logistic regression\")\n",
    "\tfinetune_logistic(X_train, y_train, X_test, y_test)\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300bd2b",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "\n",
    "We tune the constant $C$ (the paramter for regualization) of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_l2(X_train, y_train, X_test, y_test) -> tuple[LogisticRegression, float] :\n",
    "\tsearch = GridSearchCV(\n",
    "\t\tLogisticRegression(random_state=114, penalty='l2', solver='liblinear', max_iter=1000),\n",
    "\t\tparam_grid={\n",
    "\t\t\t'C': [11 + 0.1 * i for i in range(-10, 10)],\n",
    "\t\t},\n",
    "\t\tcv=5,\n",
    "\t\tn_jobs=-1,\n",
    "\t\tscoring='f1_macro'\n",
    "\t)\n",
    "\t\n",
    "\tstart_time = time()\n",
    "\tsearch.fit(X_train, y_train)\n",
    "\tend_time = time()\n",
    "\ttime_cost = end_time - start_time\n",
    "\t\n",
    "\tbst = search.best_estimator_\n",
    "\tprint(\"Best parameters:\", search.best_params_)\n",
    "\tevaluate(\"Logistic Regression\", bst, X_test, y_test, time_cost, eval_roc=False)\n",
    "\t\n",
    "\treturn bst, time_cost\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tX_train, y_train = load_data(\"./data/train\", \"train\")\n",
    "\tX_test, y_test = load_data(\"./data/test\", \"test\")\n",
    "\n",
    "\tlogistic_l2(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cb3a7",
   "metadata": {},
   "source": [
    "# Exploration of Neural Network\n",
    "\n",
    "The output below is one instance of traning process and may be different from the data we provide in the report. The data in the report can be found in $\\texttt{/run/CNN-test-2}$ and be checked using tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\tdef __init__(self, gamma=2, weight=None):\n",
    "\t\tsuper(FocalLoss, self).__init__()\n",
    "\t\tself.gamma = gamma\n",
    "\t\tself.weight = weight\n",
    "\n",
    "\tdef forward(self, inputs, targets):\n",
    "\t\tce_loss = nn.CrossEntropyLoss(weight=self.weight)(inputs, targets)  # 使用交叉熵损失函数计算基础损失\n",
    "\t\tpt = torch.exp(-ce_loss)  # 计算预测的概率\n",
    "\t\tfocal_loss = (1 - pt) ** self.gamma * ce_loss  # 根据Focal Loss公式计算Focal Loss\n",
    "\t\treturn focal_loss\n",
    "\t\n",
    "class LSTMCNN(nn.Module) :\n",
    "\tdef __init__ (self, n_input=9, num_classes=6) :\n",
    "\t\tsuper(LSTMCNN, self).__init__()\n",
    "\t\tself.lstm = nn.LSTM(input_size=n_input, hidden_size=32, num_layers=2, batch_first=True)\n",
    "\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(1, 64, kernel_size=5, stride=2),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tnn.Conv2d(64, 128, kernel_size=3, stride=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\t\t\tnn.Conv2d(128, 256, kernel_size=2, stride=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.AdaptiveAvgPool2d(1),\n",
    "\t\t\tnn.BatchNorm2d(256)\n",
    "\t\t)\n",
    "\t\tself.fc = nn.Sequential(\n",
    "\t\t\tnn.Linear(256, num_classes),\n",
    "\t\t\tnn.Softmax(dim=1)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x : torch.Tensor) :\n",
    "\t\tbatch_size = x.size(0)\n",
    "\t\tx, _ = self.lstm(x)\n",
    "\t\tx = x.unsqueeze(1)\n",
    "\n",
    "\t\tx = self.conv1(x)\n",
    "\n",
    "\t\tx = x.view(batch_size, -1)\n",
    "\t\tx = self.fc(x)\n",
    "\t\treturn x\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(data.Dataset) :\n",
    "\tdef __init__(self, data_path, name : str) :\n",
    "\t\tdef load_seq(seq_name) :\n",
    "\t\t\tseq = np.loadtxt(os.path.join(data_path, name, \"Inertial Signals\", f\"{seq_name}_{name}.txt\"), dtype=np.float32)\n",
    "\t\t\treturn seq.reshape(seq.shape[0], seq.shape[1], 1)\n",
    "\n",
    "\t\tacc_x = load_seq(\"body_acc_x\")\n",
    "\t\tacc_y = load_seq(\"body_acc_y\")\n",
    "\t\tacc_z = load_seq(\"body_acc_z\")\n",
    "\t\tgyro_x = load_seq(\"body_gyro_x\")\n",
    "\t\tgyro_y = load_seq(\"body_gyro_y\")\n",
    "\t\tgyro_z = load_seq(\"body_gyro_z\")\n",
    "\t\tmag_x = load_seq(\"total_acc_x\")\n",
    "\t\tmag_y = load_seq(\"total_acc_y\")\n",
    "\t\tmag_z = load_seq(\"total_acc_z\")\n",
    "\n",
    "\n",
    "\t\tself.input = np.concatenate((acc_x, acc_y, acc_z, gyro_x, gyro_y, gyro_z, mag_x, mag_y, mag_z), axis=2)\n",
    "\t\tlabels = np.loadtxt(os.path.join(data_path, name, f\"y_{name}.txt\"))\n",
    "\t\t# one-hot encoding\n",
    "\t\tself.label = sklearn.preprocessing.LabelBinarizer().fit_transform(labels)\n",
    "\t\tself.label = torch.tensor(self.label, dtype=torch.float32)\n",
    "\t\n",
    "\tdef __len__(self) :\n",
    "\t\treturn len(self.input)\n",
    "\t\n",
    "\tdef __getitem__(self, index) :\n",
    "\t\treturn self.input[index], self.label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data : DataLoader, criterion) -> tuple[float, float, float, float, float]:\n",
    "\tmodel.eval()\n",
    "\ttest_loss = 0\n",
    "\tcorrect = 0\n",
    "\tf1_tot = 0\n",
    "\tprecision_tot, recall_tot = 0, 0\n",
    "\ttot = 0\n",
    "\n",
    "\t# store y_true and y_pred for confusion matrix\n",
    "\ty_true = []\n",
    "\ty_pred = []\n",
    "\twith torch.no_grad() :\n",
    "\t\tfor data, target in test_data :\n",
    "\t\t\tdata : torch.Tensor = data.cuda(); target : torch.Tensor = target.cuda()\n",
    "\t\t\toutput : torch.Tensor = model(data)\n",
    "\t\t\ttest_loss += criterion(output, target).sum().item()\n",
    "\n",
    "\t\t\tcorrect += (output.argmax(1) == target.argmax(1)).sum().item()\n",
    "\t\t\ttot += target.shape[0]\n",
    "\n",
    "\t\t\ty_true.append(target.argmax(1).cpu().numpy())\n",
    "\t\t\ty_pred.append(output.argmax(1).cpu().numpy())\n",
    "\n",
    "\ttest_loss /= tot\n",
    "\tcorrect /= tot\n",
    "\n",
    "\ty_true = np.concatenate(y_true)\n",
    "\ty_pred = np.concatenate(y_pred)\n",
    "\tf1_tot = metrics.f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "\tprecision_tot = metrics.precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "\trecall_tot = metrics.recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "\n",
    "\treturn test_loss, correct, f1_tot, precision_tot, recall_tot\n",
    "\n",
    "def train(model : nn.Module, \n",
    "\t\t  train_data : DataLoader, test_data : DataLoader, learning_rate, epochs, logger : SummaryWriter, fig_path) :\n",
    "\t# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.78)\n",
    "\toptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\t# criterion = nn.CrossEntropyLoss()\n",
    "\tcriterion = FocalLoss()\n",
    "\n",
    "\ttrain_loss = []\n",
    "\ttrain_acc = []\n",
    "\ttrain_f1 = []\n",
    "\ttrain_precision = []\n",
    "\ttrain_recall = []\n",
    "\n",
    "\ttest_loss = []\n",
    "\ttest_acc = []\n",
    "\ttest_f1 = []\n",
    "\ttest_precision = []\n",
    "\ttest_recall = []\n",
    "\n",
    "\tfor epoch in tqdm(range(epochs)) :\n",
    "\t\tmodel.train()\n",
    "\t\tepoch_loss = 0\n",
    "\t\tcorrect = 0\n",
    "\t\tf1_tot = 0\n",
    "\t\tprecision_tot, recall_tot = 0, 0\n",
    "\t\ttot = 0\n",
    "\n",
    "\t\ty_true = []\n",
    "\t\ty_pred = []\n",
    "\n",
    "\t\tfor data, target in train_data :\n",
    "\t\t\tdata : torch.Tensor = data.cuda(); target : torch.Tensor = target.cuda()\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutput : torch.Tensor = model(data)\n",
    "\t\t\tloss = criterion(output, target)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\tepoch_loss += loss.sum().item()\n",
    "\t\t\tcorrect += (output.argmax(1) == target.argmax(1)).sum().item()\n",
    "\t\t\ttot += target.shape[0]\n",
    "\n",
    "\t\t\ty_true.append(target.argmax(1).cpu().numpy())\n",
    "\t\t\ty_pred.append(output.argmax(1).cpu().numpy())\n",
    "\n",
    "\t\tepoch_loss /= tot\n",
    "\t\tcorrect /= tot\n",
    "\t\t\n",
    "\t\ty_true = np.concatenate(y_true)\n",
    "\t\ty_pred = np.concatenate(y_pred)\n",
    "\n",
    "\t\tf1_tot = metrics.f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "\t\tprecision_tot = metrics.precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "\t\trecall_tot = metrics.recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "\n",
    "\t\ttest_loss_, test_acc_, test_f1_, test_precision_, test_recall_ = test(model, test_data, criterion)\n",
    "\n",
    "\t\t# print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {epoch_loss:.4f}, Train Acc: {correct:.4f}, Test Loss: {test_loss_:.4f}, Test Acc: {test_acc_:.4f}\")\n",
    "\n",
    "\t\ttrain_loss.append(epoch_loss)\n",
    "\t\ttrain_acc.append(correct)\n",
    "\t\ttrain_f1.append(f1_tot)\n",
    "\t\ttrain_precision.append(precision_tot)\n",
    "\t\ttrain_recall.append(recall_tot)\n",
    "\n",
    "\t\ttest_loss.append(test_loss_)\n",
    "\t\ttest_acc.append(test_acc_)\n",
    "\t\ttest_f1.append(test_f1_)\n",
    "\t\ttest_precision.append(test_precision_)\n",
    "\t\ttest_recall.append(test_recall_)\n",
    "\n",
    "\n",
    "\t\tlogger.add_scalar(\"train/loss\", epoch_loss, epoch + 1)\n",
    "\t\tlogger.add_scalar(\"train/acc\", correct, epoch + 1)\n",
    "\t\tlogger.add_scalar(\"train/f1\", f1_tot, epoch + 1)\n",
    "\t\tlogger.add_scalar(\"train/precision\", precision_tot, epoch + 1)\n",
    "\t\tlogger.add_scalar(\"train/recall\", recall_tot, epoch + 1)\n",
    "\n",
    "\t\tlogger.add_scalar(\"test/loss\", test_loss_, epoch + 1)\n",
    "\t\tlogger.add_scalar(\"test/acc\", test_acc_, epoch + 1)\n",
    "\t\tlogger.add_scalar(\"test/f1\", test_f1_, epoch + 1)\n",
    "\t\tlogger.add_scalar(\"test/precision\", test_precision_, epoch + 1)\n",
    "\t\tlogger.add_scalar(\"test/recall\", test_recall_, epoch + 1)\n",
    "\n",
    "\tfig, ax = plt.subplots(1, 5, figsize=(20, 8))\n",
    "\tax[0].plot(range(1, epochs + 1), train_loss, label='Train Loss')\n",
    "\tax[0].plot(range(1, epochs + 1), test_loss, label='Test Loss')\n",
    "\tax[0].set_xlabel('Epochs')\n",
    "\tax[0].set_ylabel('Loss')\n",
    "\tax[0].legend()\n",
    "\n",
    "\tax[1].plot(range(1, epochs + 1), train_acc, label='Train Accuracy')\n",
    "\tax[1].plot(range(1, epochs + 1), test_acc, label='Test Accuracy')\n",
    "\tax[1].set_xlabel('Epochs')\n",
    "\tax[1].set_ylabel('Accuracy')\n",
    "\tax[1].set_ylim(0.5, 1)\n",
    "\tax[1].legend()\n",
    "\n",
    "\tax[2].plot(range(1, epochs + 1), train_f1, label='Train F1 Score')\n",
    "\tax[2].plot(range(1, epochs + 1), test_f1, label='Test F1 Score')\n",
    "\tax[2].set_xlabel('Epochs')\n",
    "\tax[2].set_ylabel('F1 Score')\n",
    "\tax[2].set_ylim(0.5, 1)\n",
    "\tax[2].legend()\n",
    "\n",
    "\tax[3].plot(range(1, epochs + 1), train_precision, label='Train Precision')\n",
    "\tax[3].plot(range(1, epochs + 1), test_precision, label='Test Precision')\n",
    "\tax[3].set_xlabel('Epochs')\n",
    "\tax[3].set_ylabel('Precision')\n",
    "\tax[3].set_ylim(0.5, 1)\n",
    "\n",
    "\tax[3].legend()\n",
    "\n",
    "\tax[4].plot(range(1, epochs + 1), train_recall, label='Train Recall')\n",
    "\tax[4].plot(range(1, epochs + 1), test_recall, label='Test Recall')\n",
    "\tax[4].set_xlabel('Epochs')\n",
    "\tax[4].set_ylabel('Recall')\n",
    "\tax[4].set_ylim(0.5, 1)\n",
    "\n",
    "\tax[4].legend()\n",
    "\n",
    "\tplt.show()\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "\tmodel = LSTMCNN().cuda()\n",
    "\ttrain_data = DataLoader(DataSet(\"./data\", \"train\"), batch_size=100, shuffle=True, num_workers=2, drop_last=False)\n",
    "\ttest_data = DataLoader(DataSet(\"./data\", \"test\"), batch_size=100, shuffle=True, num_workers=2, drop_last=False)\n",
    "\tlogger = SummaryWriter(f\"./run/lstmcnn\", flush_secs=2)\n",
    "\n",
    "\ttrain(model, train_data, test_data, 1e-3, 450, logger, \"lstmcnn.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSAA2011",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
